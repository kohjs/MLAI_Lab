{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.applications.inception_v3 import InceptionV3\n", "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n", "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n", "from tensorflow.keras.optimizers import Adam\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "import os\n", "import shutil\n", "import random\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_random_split_directories(source_dirs, test_split=0.2, random_seed=None):\n", "    \"\"\"\n", "    Creates temporary train/test directories with random split from multiple source directories\n", "    \"\"\"\n", "    if random_seed is None:\n", "        random_seed = random.randint(1, 10000)  # Different seed each time\n", "    \n", "    random.seed(random_seed)\n", "    np.random.seed(random_seed)\n", "    \n", "    # Create temporary directories\n", "    temp_train_dir = 'temp_train'\n", "    temp_test_dir = 'temp_test'\n", "    \n", "    # Clean up existing temp directories\n", "    if os.path.exists(temp_train_dir):\n", "        shutil.rmtree(temp_train_dir)\n", "    if os.path.exists(temp_test_dir):\n", "        shutil.rmtree(temp_test_dir)\n", "    \n", "    os.makedirs(temp_train_dir)\n", "    os.makedirs(temp_test_dir)\n", "    \n", "    print(f\"Creating random split with {test_split*100}% test data...\")\n", "    \n", "    # Process each source directory\n", "    if isinstance(source_dirs, str):\n", "        source_dirs = [source_dirs]\n", "        \n", "    # First pass: collect all class names\n", "    all_classes = set()\n", "    for source_dir in source_dirs:\n", "        all_classes.update(os.listdir(source_dir))\n", "    \n", "    # Create class directories in temp folders\n", "    for class_name in all_classes:\n", "        os.makedirs(os.path.join(temp_train_dir, class_name), exist_ok=True)\n", "        os.makedirs(os.path.join(temp_test_dir, class_name), exist_ok=True)\n", "    \n", "    # Process each class directory from all sources\n", "    for class_name in all_classes:\n", "        all_image_files = []\n", "        \n", "        for source_dir in source_dirs:\n", "            class_path = os.path.join(source_dir, class_name)\n", "            if not os.path.isdir(class_path):\n", "                continue\n", "                \n", "            # Get all image files from this source\n", "            image_files = [f for f in os.listdir(class_path)\n", "                          if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n", "            all_image_files.extend([(source_dir, f) for f in image_files])\n", "        \n", "        # Random split\n", "        random.shuffle(all_image_files)\n", "        split_idx = int(len(all_image_files) * (1 - test_split))\n", "        train_files = all_image_files[:split_idx]\n", "        test_files = all_image_files[split_idx:]\n", "        \n", "        print(f\"Class '{class_name}': {len(train_files)} train, {len(test_files)} test images\")\n", "        \n", "        # Copy files to temporary directories\n", "        for source_dir, file in train_files:\n", "            src = os.path.join(source_dir, class_name, file)\n", "            dst = os.path.join(temp_train_dir, class_name, file)\n", "            shutil.copy2(src, dst)\n", "            \n", "        for source_dir, file in test_files:\n", "            src = os.path.join(source_dir, class_name, file)\n", "            dst = os.path.join(temp_test_dir, class_name, file)\n", "            shutil.copy2(src, dst)\n", "    \n", "    print(f\"Random split created with seed: {random_seed}\")\n", "    return temp_train_dir, temp_test_dir, random_seed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def improved_image_gen_w_random_split(source_directories, test_split=0.2, random_seed=None):\n", "    \"\"\"\n", "    Enhanced data augmentation with random train/test split from multiple sources\n", "    \"\"\"\n", "    # Create random split directories\n", "    temp_train_dir, temp_test_dir, used_seed = create_random_split_directories(\n", "        source_directories, test_split, random_seed\n", "    )\n", "    \n", "    train_datagen = ImageDataGenerator(\n", "        rescale=1/255,\n", "        horizontal_flip=True,         # Still useful\n", "        shear_range=0.1,              # Slightly reduce\n", "        channel_shift_range=0.05,     # Reduce intensity\n", "        validation_split=0.15        # Larger validation split for small datasets\n", "    )\n", "    test_datagen = ImageDataGenerator(rescale=1/255)\n\n", "    # Improved batch sizes for better gradient estimates\n", "    train_generator = train_datagen.flow_from_directory(\n", "        temp_train_dir,\n", "        target_size=(150, 150),      # Increased input size for better features\n", "        batch_size=32,               # Standard batch size\n", "        class_mode='categorical',\n", "        subset='training',\n", "        shuffle=True\n", "    )\n", "    val_generator = train_datagen.flow_from_directory(\n", "        temp_train_dir,\n", "        target_size=(150, 150),\n", "        batch_size=32,\n", "        class_mode='categorical',\n", "        subset='validation',\n", "        shuffle=False\n", "    )\n", "    test_generator = test_datagen.flow_from_directory(\n", "        temp_test_dir,\n", "        target_size=(150, 150),\n", "        batch_size=32,\n", "        class_mode='categorical',\n", "        shuffle=False\n", "    )\n", "    return train_generator, val_generator, test_generator, used_seed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def improved_model_output_for_TL(pre_trained_model, last_output, num_classes=3):\n", "    \"\"\"Improved model head with better architecture\"\"\"\n", "    # Use GlobalAveragePooling instead of Flatten to reduce parameters\n", "    x = GlobalAveragePooling2D()(last_output)\n", "    \n", "    # Add batch normalization for stability\n", "    x = BatchNormalization()(x)\n", "    \n", "    # First dense layer with L2 regularization and reduced size\n", "    x = Dense(256, activation='relu', kernel_regularizer='l2')(x)\n", "    x = BatchNormalization()(x)\n", "    x = Dropout(0.6)(x)\n", "    \n", "    # Second dense layer with L2 regularization and reduced size\n", "    x = Dense(128, activation='relu', kernel_regularizer='l2')(x)\n", "    x = BatchNormalization()(x)\n", "    x = Dropout(0.5)(x)\n", "    \n", "    # Output layer\n", "    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n", "    \n", "    model = Model(pre_trained_model.input, predictions)\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_callbacks():\n", "    \"\"\"Create useful callbacks for training\"\"\"\n", "    callbacks = [\n", "        EarlyStopping(\n", "                monitor='val_loss',\n", "                patience=10,  # Increased patience to allow more regularization\n", "            restore_best_weights=True,\n", "            verbose=1\n", "        ),\n", "        ModelCheckpoint(\n", "            'backend/model/improved_inception_model.keras',\n", "            monitor='val_accuracy',\n", "            save_best_only=True,\n", "            verbose=1\n", "        ),\n", "        ReduceLROnPlateau(\n", "            monitor='val_loss',\n", "            factor=0.2,\n", "            patience=5,\n", "            min_lr=1e-7,\n", "            verbose=1\n", "        )\n", "    ]\n", "    return callbacks"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def fine_tune_model(model, train_generator, validation_generator, unfreeze_layers=50):\n", "    \"\"\"Fine-tune the model by unfreezing some layers\"\"\"\n", "    # Unfreeze the top layers of the pre-trained model\n", "    for layer in model.layers[-unfreeze_layers:]:\n", "        if not isinstance(layer, BatchNormalization):  # Keep BN layers frozen\n", "            layer.trainable = True\n", "    \n", "    # Recompile with a lower learning rate for fine-tuning\n", "    model.compile(\n", "        optimizer=Adam(learning_rate=0.000001),  # Lower learning rate for fine-tuning\n", "        loss='categorical_crossentropy',\n", "        metrics=['accuracy']\n", "    )\n", "    \n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cleanup_temp_directories():\n", "    \"\"\"Clean up temporary directories after training\"\"\"\n", "    temp_dirs = ['temp_train', 'temp_test']\n", "    for temp_dir in temp_dirs:\n", "        if os.path.exists(temp_dir):\n", "            shutil.rmtree(temp_dir)\n", "            print(f\"Cleaned up {temp_dir}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Main training script with random splitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_improved_model_with_random_split():\n", "    # IMPORTANT: Change these to your dataset directories\n", "    # These should contain subdirectories for each class with all your images\n", "    source_dirs = [\n", "        'C:/MLAI_Lab/all_data/',\n", "        'C:/MLAI_Lab/augmented_data/'\n", "    ]\n", "    \n", "    # Check if source directories exist\n", "    for source_dir in source_dirs:\n", "        if not os.path.exists(source_dir):\n", "            print(f\"ERROR: Source directory '{source_dir}' does not exist!\")\n", "            print(\"Please update the source_dirs paths to point to your datasets.\")\n", "            print(\"Your directory structure should look like:\")\n", "            print(\"C:/MLAI_Lab/all_data/\")\n", "            print(\"\u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac pancake/\")\n", "            print(\"\u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac pancake1.jpg\")\n", "            print(\"\u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac pancake2.jpg\")\n", "            print(\"\u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac ...\")\n", "            print(\"\u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac strawberry/\")\n", "            print(\"\u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac strawberry1.jpg\")\n", "            print(\"\u00e2\u201d\u201a   \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac strawberry2.jpg\")\n", "            print(\"\u00e2\u201d\u201a   \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac ...\")\n", "            print(\"\u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac none/\")\n", "            print(\"    \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac other1.jpg\")\n", "            print(\"    \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac other2.jpg\")\n", "            print(\"    \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac ... (images that are neither pancake nor strawberry)\")\n", "            print(\"\\nAnd similarly for augmented_data/ directory\")\n", "            return None, None, None, None\n", "    \n", "    try:\n", "        # Generate improved image data with random split\n", "        train_generator, validation_generator, test_generator, used_seed = improved_image_gen_w_random_split(\n", "            source_dirs,\n", "            test_split=0.2,     # 20% for testing, 80% for training\n", "            random_seed=None    # None = different split each time, or set a number for reproducible splits\n", "        )\n\n", "        # Load pre-trained InceptionV3 with improved input size\n", "        pre_trained_model = InceptionV3(\n", "            input_shape=(150, 150, 3),   # Larger input size\n", "            include_top=False,\n", "            weights='imagenet'\n", "        )\n\n", "        # Freeze all layers initially\n", "        for layer in pre_trained_model.layers:\n", "            layer.trainable = False\n\n", "        # Use a later layer for better feature extraction\n", "        last_layer = pre_trained_model.get_layer('mixed7')  # Deeper layer\n", "        last_output = last_layer.output\n\n", "        # Create improved model\n", "        model_TL = improved_model_output_for_TL(pre_trained_model, last_output)\n\n", "        # Compile with better optimizer settings\n", "        model_TL.compile(\n", "            optimizer=Adam(learning_rate=0.0001),\n", "            loss='categorical_crossentropy',\n", "            metrics=['accuracy']\n", "        )\n\n", "        # Create callbacks\n", "        callbacks = create_callbacks()\n\n", "        # Calculate steps per epoch properly\n", "        steps_per_epoch = train_generator.samples // train_generator.batch_size\n", "        validation_steps = validation_generator.samples // validation_generator.batch_size\n", "        print(f\"Steps per epoch: {steps_per_epoch}\")\n", "        print(f\"Validation steps: {validation_steps}\")\n", "        print(f\"Training with random seed: {used_seed}\")\n\n", "        # Phase 1: Train with frozen base\n", "        print(\"Phase 1: Training with frozen base model...\")\n", "        history1 = model_TL.fit(\n", "            train_generator,\n", "            steps_per_epoch=steps_per_epoch,\n", "            epochs=20,\n", "            validation_data=validation_generator,\n", "            validation_steps=validation_steps,\n", "            callbacks=callbacks,\n", "            verbose=1\n", "        )\n\n", "        # Phase 2: Fine-tune with unfrozen layers\n", "        print(\"Phase 2: Fine-tuning with unfrozen layers...\")\n", "        model_TL = fine_tune_model(model_TL, train_generator, validation_generator)\n", "        \n", "        # Reset callbacks for fine-tuning\n", "        callbacks = create_callbacks()\n", "        \n", "        history2 = model_TL.fit(\n", "            train_generator,\n", "            steps_per_epoch=steps_per_epoch,\n", "            epochs=15,\n", "            validation_data=validation_generator,\n", "            validation_steps=validation_steps,\n", "            callbacks=callbacks,\n", "            verbose=1\n", "        )\n\n", "        # Combine histories\n", "        history = {\n", "            'accuracy': history1.history['accuracy'] + history2.history['accuracy'],\n", "            'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy'],\n", "            'loss': history1.history['loss'] + history2.history['loss'],\n", "            'val_loss': history1.history['val_loss'] + history2.history['val_loss']\n", "        }\n", "        return model_TL, history, test_generator, used_seed\n", "    \n", "    except Exception as e:\n", "        print(f\"Error during training: {e}\")\n", "        # Clean up in case of error\n", "        cleanup_temp_directories()\n", "        raise e"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_training_history(history):\n", "    \"\"\"Enhanced plotting function\"\"\"\n", "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n", "    \n", "    # Accuracy plot\n", "    axes[0, 0].plot(history['accuracy'], label='Train Accuracy', linewidth=2)\n", "    axes[0, 0].plot(history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n", "    axes[0, 0].set_title('Model Accuracy', fontsize=14)\n", "    axes[0, 0].set_xlabel('Epoch')\n", "    axes[0, 0].set_ylabel('Accuracy')\n", "    axes[0, 0].legend()\n", "    axes[0, 0].grid(True, alpha=0.3)\n", "    \n", "    # Loss plot\n", "    axes[0, 1].plot(history['loss'], label='Train Loss', linewidth=2)\n", "    axes[0, 1].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n", "    axes[0, 1].set_title('Model Loss', fontsize=14)\n", "    axes[0, 1].set_xlabel('Epoch')\n", "    axes[0, 1].set_ylabel('Loss')\n", "    axes[0, 1].legend()\n", "    axes[0, 1].grid(True, alpha=0.3)\n", "    \n", "    # Learning rate plot (if available)\n", "    axes[1, 0].set_title('Training Phases')\n", "    axes[1, 0].text(0.1, 0.8, 'Phase 1: Frozen base (Epochs 1-20)', transform=axes[1, 0].transAxes)\n", "    axes[1, 0].text(0.1, 0.6, 'Phase 2: Fine-tuning (Epochs 21-35)', transform=axes[1, 0].transAxes)\n", "    axes[1, 0].set_xticks([])\n", "    axes[1, 0].set_yticks([])\n", "    \n", "    # Validation accuracy vs training accuracy difference\n", "    val_train_diff = [val - train for val, train in zip(history['val_accuracy'], history['accuracy'])]\n", "    axes[1, 1].plot(val_train_diff, linewidth=2, color='red')\n", "    axes[1, 1].set_title('Validation - Training Accuracy')\n", "    axes[1, 1].set_xlabel('Epoch')\n", "    axes[1, 1].set_ylabel('Accuracy Difference')\n", "    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n", "    axes[1, 1].grid(True, alpha=0.3)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_model(model, test_generator):\n", "    \"\"\"Comprehensive model evaluation\"\"\"\n", "    print(\"Evaluating model on test set...\")\n", "    test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n", "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n", "    print(f\"Test Loss: {test_loss:.4f}\")\n", "    \n", "    return test_accuracy, test_loss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the improved training with random splitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    print(\"Starting training with random train/test split...\")\n", "    \n", "    try:\n", "        # Train the model\n", "        model, history, test_gen, seed = train_improved_model_with_random_split()\n", "        \n", "        if model is not None:\n", "            # Plot training history\n", "            plot_training_history(history)\n", "            \n", "            # Evaluate on test set\n", "            evaluate_model(model, test_gen)\n", "            \n", "            # Save the final model\n", "            model_filename = 'backend/model/improved_inception_model.keras'\n", "            model.save(model_filename)\n", "            print(f\"Model saved as '{model_filename}'\")\n", "            \n", "            print(f\"Training completed successfully with random seed: {seed}\")\n", "            print(\"Note: Each training run will use a different random split of your data.\")\n", "        \n", "    except Exception as e:\n", "        print(f\"Training failed: {e}\")\n", "    \n", "    finally:\n", "        # Always clean up temporary directories\n", "        cleanup_temp_directories()\n", "        if model is not None:\n", "            print(\"\\nFinal Model Architecture:\")\n", "            model.summary()\n", "        print(\"Training session ended.\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}